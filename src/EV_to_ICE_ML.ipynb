{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stas420/machine_learning_ev_ice/blob/main/src/EV_to_ICE_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup and dependencies\n",
        "Run the below cell, which shall clone the repo contents and prepare the machine for work. This may take a few minutes.\n",
        "\n",
        "***Important:*** This is the first cell you shall run in this project, otherwise it won't be working.\n",
        "\n",
        "After 'pip' is done, it may ask you to restart this session - please do accordingly."
      ],
      "metadata": {
        "id": "PRPDslf-MuH0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oB99vcFrGWR",
        "outputId": "e5ccf7fa-36ff-44fb-e025-a5290b649277"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'machine_learning_ev_ice'...\n",
            "remote: Enumerating objects: 80, done.\u001b[K\n",
            "remote: Counting objects: 100% (80/80), done.\u001b[K\n",
            "remote: Compressing objects: 100% (62/62), done.\u001b[K\n",
            "remote: Total 80 (delta 39), reused 46 (delta 15), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (80/80), 25.58 MiB | 30.46 MiB/s, done.\n",
            "Resolving deltas: 100% (39/39), done.\n",
            "/content/machine_learning_ev_ice\n",
            "Already up to date.\n",
            "/content/machine_learning_ev_ice/src\n",
            "Requirement already satisfied: setuptools==78.1.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (78.1.0)\n",
            "Requirement already satisfied: labelImg==1.8.6 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (1.8.6)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy==2.2.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (2.2.4)\n",
            "Requirement already satisfied: ultralytics==8.3.140 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (8.3.140)\n",
            "Requirement already satisfied: opencv-python-headless==4.11.0.86 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (4.11.0.86)\n",
            "Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (4.67.1)\n",
            "Requirement already satisfied: pyqt5 in /usr/local/lib/python3.11/dist-packages (from labelImg==1.8.6->-r requirements.txt (line 3)) (5.15.11)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from labelImg==1.8.6->-r requirements.txt (line 3)) (5.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->-r requirements.txt (line 6)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->-r requirements.txt (line 6)) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->-r requirements.txt (line 6)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->-r requirements.txt (line 6)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->-r requirements.txt (line 6)) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->-r requirements.txt (line 6)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->-r requirements.txt (line 6)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->-r requirements.txt (line 6)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->-r requirements.txt (line 6)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->-r requirements.txt (line 6)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->-r requirements.txt (line 6)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->-r requirements.txt (line 6)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->-r requirements.txt (line 6)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->-r requirements.txt (line 6)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->-r requirements.txt (line 6)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->-r requirements.txt (line 6)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->-r requirements.txt (line 6)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->-r requirements.txt (line 6)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->-r requirements.txt (line 6)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->-r requirements.txt (line 6)) (1.13.1)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.140->-r requirements.txt (line 8)) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.140->-r requirements.txt (line 8)) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.140->-r requirements.txt (line 8)) (11.2.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.140->-r requirements.txt (line 8)) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.140->-r requirements.txt (line 8)) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.140->-r requirements.txt (line 8)) (1.15.3)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.140->-r requirements.txt (line 8)) (0.21.0+cu124)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.140->-r requirements.txt (line 8)) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.140->-r requirements.txt (line 8)) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.140->-r requirements.txt (line 8)) (2.2.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.140->-r requirements.txt (line 8)) (2.0.14)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->-r requirements.txt (line 6)) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.140->-r requirements.txt (line 8)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.140->-r requirements.txt (line 8)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.140->-r requirements.txt (line 8)) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.140->-r requirements.txt (line 8)) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.140->-r requirements.txt (line 8)) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.140->-r requirements.txt (line 8)) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.140->-r requirements.txt (line 8)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics==8.3.140->-r requirements.txt (line 8)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics==8.3.140->-r requirements.txt (line 8)) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics==8.3.140->-r requirements.txt (line 8)) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics==8.3.140->-r requirements.txt (line 8)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics==8.3.140->-r requirements.txt (line 8)) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics==8.3.140->-r requirements.txt (line 8)) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->-r requirements.txt (line 6)) (3.0.2)\n",
            "Requirement already satisfied: PyQt5-sip<13,>=12.15 in /usr/local/lib/python3.11/dist-packages (from pyqt5->labelImg==1.8.6->-r requirements.txt (line 3)) (12.17.0)\n",
            "Requirement already satisfied: PyQt5-Qt5<5.16.0,>=5.15.2 in /usr/local/lib/python3.11/dist-packages (from pyqt5->labelImg==1.8.6->-r requirements.txt (line 3)) (5.15.16)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics==8.3.140->-r requirements.txt (line 8)) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# remove possible leftovers and clear-clone the whole repo\n",
        "%cd /content\n",
        "%rm -rf ./machine_learning_ev_ice\n",
        "!git clone https://github.com/stas420/machine_learning_ev_ice.git\n",
        "\n",
        "# a git pull pro forma\n",
        "%cd ./machine_learning_ev_ice\n",
        "!git pull\n",
        "\n",
        "# go to our main directory\n",
        "%cd ./src\n",
        "\n",
        "# check (and download, if needed) dependencies for the project\n",
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# małnt dejta\n",
        "\n",
        "tu sb poberamy dane z podmontowanego dysku gugl"
      ],
      "metadata": {
        "id": "ipp6kZ_9NTTK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IeZ7a9crGWT",
        "outputId": "a9365e85-4094-4961-c814-2377d8620d9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "if not 'drive' in os.listdir():\n",
        "  drive.mount('/content/drive')\n",
        "else:\n",
        "  print(\"drajw dajrektory alredi prezent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a tu sb dodamy zmienne globalne UwU"
      ],
      "metadata": {
        "id": "WQpmgashBupp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VIDEOS_PATH = '/content/drive/MyDrive/videos'\n",
        "IMAGES_PATH = '/content/imgs'"
      ],
      "metadata": {
        "id": "VTmr5b5lBuOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# extrakt frejms"
      ],
      "metadata": {
        "id": "F5sCM44OjlQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clearing leftovers pro forma\n",
        "%rm -rf /content/imgs\n",
        "%mkdir /content/imgs\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# function which takes a .mp4 file and extracts its frames,\n",
        "# then saves them as images in the provided directory\n",
        "def extract_frames(video_path, output_dir, fps=1) -> None:\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    count = 0\n",
        "    saved = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        else:\n",
        "            frame_path = os.path.join(output_dir, f\"frame_{saved:03d}.jpg\")\n",
        "            cv2.imwrite(frame_path, frame)\n",
        "            saved += 1\n",
        "        count += 1\n",
        "    cap.release()\n",
        "\n",
        "# image extraction\n",
        "for x in os.listdir(VIDEOS_PATH):\n",
        "  extract_frames(VIDEOS_PATH + '/' + x, \"/content/imgs/\" + x, fps=1)\n",
        "\n",
        "!ls -la ./content/imgs"
      ],
      "metadata": {
        "id": "eBpUp2Lxjm1R",
        "outputId": "8155fa73-adb7-47fc-979d-215dd0a792e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-69b9ebd8e8bd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# image extraction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVIDEOS_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m   \u001b[0mextract_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVIDEOS_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/content/imgs/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ls -la ./content/imgs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-69b9ebd8e8bd>\u001b[0m in \u001b[0;36mextract_frames\u001b[0;34m(video_path, output_dir, fps)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misOpened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# testujemy JOLO"
      ],
      "metadata": {
        "id": "BGA7EFkJgUZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/machine_learning_ev_ice/src\n",
        "\n",
        "# for development only, suck my ass!\n",
        "%rm -rf /content/results\n",
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "def is_green_plate(plate_img):\n",
        "    hsv = cv2.cvtColor(plate_img, cv2.COLOR_BGR2HSV)\n",
        "    h, s, _ = hsv.mean(axis=0).mean(axis=0)\n",
        "    return 35 < h < 85 and s > 40\n",
        "\n",
        "model = YOLO('./yolov8s.pt')\n",
        "input_dir = \"/content/imgs/2024-02-08.mp4\"\n",
        "output_dir = \"/content/results\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# for each file, check if it's a jpg and pass it to the model\n",
        "for fname in os.listdir(input_dir):\n",
        "    if not fname.endswith(\".jpg\"):\n",
        "        continue\n",
        "    path = os.path.join(input_dir, fname)\n",
        "    img = cv2.imread(path)\n",
        "    results = model(img)\n",
        "\n",
        "    # for each 'box' in obtained result write a frame around the found object\n",
        "    for box in results[0].boxes:\n",
        "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "        crop = img[y1:y2, x1:x2]\n",
        "        class_id = int(box.cls)\n",
        "        label = model.names[class_id]\n",
        "        color = (0,255,0) # if label == \"green\" else (255,0,0)\n",
        "\n",
        "        cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
        "        cv2.putText(img, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
        "\n",
        "    cv2.imwrite(os.path.join(output_dir, fname), img)\n"
      ],
      "metadata": {
        "id": "pRG7YckugWgM",
        "outputId": "f397a95e-f3ce-4db0-8503-be3579ee1b1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/machine_learning_ev_ice/src\n",
            "\n",
            "0: 384x640 8 cars, 1 bus, 1 truck, 663.0ms\n",
            "Speed: 4.1ms preprocess, 663.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 truck, 643.1ms\n",
            "Speed: 4.0ms preprocess, 643.1ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 3 buss, 1 truck, 630.1ms\n",
            "Speed: 5.1ms preprocess, 630.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 bus, 1 truck, 633.0ms\n",
            "Speed: 4.4ms preprocess, 633.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 1 bus, 2 trucks, 1 traffic light, 489.0ms\n",
            "Speed: 6.4ms preprocess, 489.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 1 truck, 1 traffic light, 414.2ms\n",
            "Speed: 4.7ms preprocess, 414.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 416.3ms\n",
            "Speed: 4.3ms preprocess, 416.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 1 truck, 1 traffic light, 403.6ms\n",
            "Speed: 4.1ms preprocess, 403.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 1 traffic light, 426.1ms\n",
            "Speed: 4.2ms preprocess, 426.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 24 cars, 1 traffic light, 408.2ms\n",
            "Speed: 3.9ms preprocess, 408.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 400.2ms\n",
            "Speed: 3.8ms preprocess, 400.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1 truck, 416.9ms\n",
            "Speed: 3.8ms preprocess, 416.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 1 traffic light, 408.2ms\n",
            "Speed: 4.0ms preprocess, 408.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 truck, 418.3ms\n",
            "Speed: 4.1ms preprocess, 418.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 9 cars, 1 truck, 402.2ms\n",
            "Speed: 4.0ms preprocess, 402.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 1 truck, 1 traffic light, 409.3ms\n",
            "Speed: 3.8ms preprocess, 409.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 13 cars, 1 bus, 1 truck, 1 traffic light, 443.0ms\n",
            "Speed: 3.9ms preprocess, 443.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 truck, 419.0ms\n",
            "Speed: 4.5ms preprocess, 419.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 cars, 1 traffic light, 441.2ms\n",
            "Speed: 4.0ms preprocess, 441.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 cars, 2 trucks, 1 traffic light, 415.7ms\n",
            "Speed: 4.0ms preprocess, 415.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 truck, 423.6ms\n",
            "Speed: 4.1ms preprocess, 423.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1 bus, 1 truck, 2 traffic lights, 407.5ms\n",
            "Speed: 4.2ms preprocess, 407.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 18 cars, 1 bus, 1 traffic light, 403.4ms\n",
            "Speed: 4.3ms preprocess, 403.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 13 cars, 427.8ms\n",
            "Speed: 4.3ms preprocess, 427.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 2 buss, 1 truck, 415.8ms\n",
            "Speed: 3.1ms preprocess, 415.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 22 cars, 1 traffic light, 460.4ms\n",
            "Speed: 4.6ms preprocess, 460.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 cars, 1 bus, 1 traffic light, 403.8ms\n",
            "Speed: 4.0ms preprocess, 403.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 2 buss, 1 truck, 646.5ms\n",
            "Speed: 4.5ms preprocess, 646.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 22 cars, 620.6ms\n",
            "Speed: 4.2ms preprocess, 620.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 1 bus, 1 truck, 625.2ms\n",
            "Speed: 4.8ms preprocess, 625.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 bus, 1 truck, 642.7ms\n",
            "Speed: 4.3ms preprocess, 642.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 22 cars, 1 truck, 1 traffic light, 629.3ms\n",
            "Speed: 4.2ms preprocess, 629.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 1 motorcycle, 1 bus, 642.5ms\n",
            "Speed: 4.0ms preprocess, 642.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 1 truck, 537.4ms\n",
            "Speed: 3.8ms preprocess, 537.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 cars, 1 truck, 419.1ms\n",
            "Speed: 4.6ms preprocess, 419.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 1 traffic light, 419.1ms\n",
            "Speed: 4.0ms preprocess, 419.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 423.0ms\n",
            "Speed: 4.2ms preprocess, 423.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1 truck, 405.7ms\n",
            "Speed: 4.2ms preprocess, 405.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 33 cars, 1 truck, 1 traffic light, 407.1ms\n",
            "Speed: 4.0ms preprocess, 407.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 truck, 431.9ms\n",
            "Speed: 4.3ms preprocess, 431.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 410.9ms\n",
            "Speed: 5.0ms preprocess, 410.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 12 cars, 1 traffic light, 433.4ms\n",
            "Speed: 4.6ms preprocess, 433.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 1 bus, 1 truck, 440.1ms\n",
            "Speed: 4.2ms preprocess, 440.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 12 cars, 2 trucks, 444.2ms\n",
            "Speed: 4.8ms preprocess, 444.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 10 cars, 1 truck, 415.8ms\n",
            "Speed: 4.4ms preprocess, 415.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 truck, 405.4ms\n",
            "Speed: 4.1ms preprocess, 405.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 20 cars, 2 buss, 1 truck, 1 traffic light, 423.8ms\n",
            "Speed: 4.4ms preprocess, 423.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 23 cars, 1 traffic light, 405.4ms\n",
            "Speed: 4.9ms preprocess, 405.4ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 427.0ms\n",
            "Speed: 4.5ms preprocess, 427.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1 bus, 1 truck, 409.0ms\n",
            "Speed: 3.8ms preprocess, 409.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 1 traffic light, 427.6ms\n",
            "Speed: 4.8ms preprocess, 427.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 cars, 1 traffic light, 419.6ms\n",
            "Speed: 4.3ms preprocess, 419.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 2 buss, 1 truck, 407.5ms\n",
            "Speed: 4.1ms preprocess, 407.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 truck, 429.3ms\n",
            "Speed: 4.4ms preprocess, 429.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 407.0ms\n",
            "Speed: 4.1ms preprocess, 407.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 truck, 1 traffic light, 422.8ms\n",
            "Speed: 3.8ms preprocess, 422.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 cars, 1 traffic light, 607.5ms\n",
            "Speed: 4.2ms preprocess, 607.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 truck, 667.3ms\n",
            "Speed: 4.1ms preprocess, 667.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 27 cars, 1 bus, 640.9ms\n",
            "Speed: 4.8ms preprocess, 640.9ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 bus, 656.8ms\n",
            "Speed: 5.4ms preprocess, 656.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1 bus, 1 truck, 642.2ms\n",
            "Speed: 4.3ms preprocess, 642.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1 motorcycle, 2 buss, 1 truck, 655.8ms\n",
            "Speed: 4.6ms preprocess, 655.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 truck, 538.5ms\n",
            "Speed: 5.6ms preprocess, 538.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 bicycle, 21 cars, 1 motorcycle, 1 traffic light, 407.4ms\n",
            "Speed: 3.9ms preprocess, 407.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 2 buss, 1 truck, 417.6ms\n",
            "Speed: 4.8ms preprocess, 417.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 cars, 2 trucks, 1 traffic light, 408.4ms\n",
            "Speed: 3.8ms preprocess, 408.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 truck, 428.3ms\n",
            "Speed: 3.8ms preprocess, 428.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 1 traffic light, 394.4ms\n",
            "Speed: 4.1ms preprocess, 394.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 16 cars, 1 bus, 1 traffic light, 403.1ms\n",
            "Speed: 3.8ms preprocess, 403.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 2 buss, 1 suitcase, 418.8ms\n",
            "Speed: 3.8ms preprocess, 418.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 30 cars, 1 truck, 1 traffic light, 402.6ms\n",
            "Speed: 3.9ms preprocess, 402.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 23 cars, 1 traffic light, 419.1ms\n",
            "Speed: 3.8ms preprocess, 419.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 1 bus, 1 truck, 403.2ms\n",
            "Speed: 5.0ms preprocess, 403.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 cars, 1 truck, 406.3ms\n",
            "Speed: 4.3ms preprocess, 406.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 bus, 1 truck, 404.8ms\n",
            "Speed: 4.0ms preprocess, 404.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 2 buss, 1 truck, 406.5ms\n",
            "Speed: 4.2ms preprocess, 406.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 truck, 413.8ms\n",
            "Speed: 4.0ms preprocess, 413.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 16 cars, 1 bus, 419.0ms\n",
            "Speed: 4.4ms preprocess, 419.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 3 buss, 1 truck, 423.8ms\n",
            "Speed: 4.4ms preprocess, 423.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 bus, 1 truck, 1 traffic light, 406.0ms\n",
            "Speed: 4.2ms preprocess, 406.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 bus, 1 truck, 398.6ms\n",
            "Speed: 4.4ms preprocess, 398.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1 bus, 2 trucks, 426.1ms\n",
            "Speed: 4.7ms preprocess, 426.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 25 cars, 1 traffic light, 420.2ms\n",
            "Speed: 4.4ms preprocess, 420.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 22 cars, 1 traffic light, 449.4ms\n",
            "Speed: 5.2ms preprocess, 449.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 1 motorcycle, 431.6ms\n",
            "Speed: 3.9ms preprocess, 431.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1 bus, 1 truck, 538.0ms\n",
            "Speed: 4.2ms preprocess, 538.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 cars, 1 bus, 1 truck, 648.7ms\n",
            "Speed: 4.7ms preprocess, 648.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 bus, 1 truck, 652.2ms\n",
            "Speed: 4.0ms preprocess, 652.2ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 bus, 1 traffic light, 624.3ms\n",
            "Speed: 3.9ms preprocess, 624.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# prawdziwe mlowanie\n"
      ],
      "metadata": {
        "id": "pWtCZg8DNPf5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OXLvE_UK2xN",
        "outputId": "c68f585d-e903-4523-fad5-b68db061cf7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9320, 0.5988, 0.1856],\n",
            "        [0.5191, 0.3469, 0.9685],\n",
            "        [0.1125, 0.9076, 0.0556],\n",
            "        [0.0212, 0.8824, 0.7880],\n",
            "        [0.8299, 0.5203, 0.5885]])\n"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# 2. Wczytanie modelu YOLO (np. wytrenowany na tablicach)\n",
        "model = YOLO(\"../models/license_plate.pt\")\n",
        "\n",
        "# 3. Wczytanie przykładowego obrazu\n",
        "image_path = \"../data/frames/frame_01.jpg\"\n",
        "image = cv2.imread(image_path)\n",
        "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# 4. Detekcja tablic rejestracyjnych\n",
        "results = model(image)\n",
        "\n",
        "# 5. Analiza i klasyfikacja kolorów tablic\n",
        "for box in results[0].boxes:\n",
        "    x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "    cropped = image[y1:y2, x1:x2]\n",
        "\n",
        "    # Analiza koloru – przejście na HSV\n",
        "    hsv = cv2.cvtColor(cropped, cv2.COLOR_BGR2HSV)\n",
        "    avg_color = hsv.mean(axis=0).mean(axis=0)\n",
        "\n",
        "    # Detekcja zielonej barwy\n",
        "    h, s, v = avg_color\n",
        "    is_green = 35 < h < 85 and s > 40\n",
        "\n",
        "    color_label = \"ZIELONA (elektryczny)\" if is_green else \"Zwykła\"\n",
        "\n",
        "    # Rysowanie wyników\n",
        "    cv2.rectangle(image, (x1, y1), (x2, y2), (0,255,0) if is_green else (255,0,0), 2)\n",
        "    cv2.putText(image, color_label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9,\n",
        "                (0,255,0) if is_green else (255,0,0), 2)\n",
        "\n",
        "# 6. Wyświetlenie wyników\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6ZA89xgozHj"
      },
      "source": [
        "Dżem dobry\n",
        ":)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}